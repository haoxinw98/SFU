## 统计学习导论(3.1)-简单线性回归
> Linear Regression serves as a good jumping-off point for many fancy statistical learning approaches that can be seen as **generalizations or extensions** of linear regression

假设笔记(2.1)中的TV广告($X$)和Sales($Y$)存在线性关系:
$$Y\approx\beta_0+\beta_1X$$  
通过估计两个未知系数就可以根据已知的TV广告费预测未来的销量 $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

### 1. 估计系数
#### 残差平方和（Residual Sum of Squares)
$$RSS= e_1^2+e_2^2+…+e_n^2$$
$$=(y_1-\hat{\beta_0}-\hat{\beta_1}x_1)^2+(y_2-\hat{\beta_0}-\hat{\beta_1}x_2)^2$$
$$+...+(y_n-\hat{\beta_0}-\hat{\beta_1}x_n)^2 = \sum_{i=1}^{n}(y_i-\hat{y_i})^2$$
**最小二乘估计**通过计算选择$\beta_0$和$\beta_1$使得RSS达到最小从而得到最优线性回归模型：
$$\hat{\beta_1}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$\beta_0=\bar{y}-\hat{\beta_1}\bar{x}$$

![](https://imgkr2.cn-bj.ufileos.com/987cf683-f6d4-49a6-a9eb-83be494d195d.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=DLRiwT9pvoDJkDhxsxqen2Bm1kM%253D&Expires=1600404409)
#### 2. 评估系数估计值的准确性
真实的关系可能不是线性的，可能是其他变量导致了Y的变化，也可能存在测量误差。
- 通常假设误差项独立于X

![](https://imgkr2.cn-bj.ufileos.com/625fe2e9-bc32-4338-9bd3-2fa2d87c16ee.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=LAFZ9hjZgq0UtK6iHhcsSIP4LK4%253D&Expires=1600404659)

真实的**population regression line**和**least squares line**的差别原因：
- The concept of these two lines is a natural extension of the standard statistical approach of using information from a **sample** to estimate characteristics of a **large population**
- e.g. The sample mean $\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$ will provide a *good* estimate of the population mean $\hat{u}$ of variable Y

> The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of **bias**  

**Unbiased Estimate**  
Use the sample mean $\hat{\mu}$ to estimate $\mu$
- in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$
- if we average a huge number of estimates of $u$ obtained from a huge number of sets of observations, then this average would **exactly equal $\mu$**
- Hence, an unbiased estimator does not systematically **over- or under-estimate** the true parameter

The property of unbiasedness holds for the least squares coefficient estimates as well:
- average the estimates obtained over a huge number of data sets, then the average of these estimates would equal to $\beta_0$ and $\beta_1$

**Question**: how *accurate* is the sample mean $\hat{\mu}$ as an estimate of $\mu$? 
- the average of $\hat{\mu}$'s over MANY data sets will be very close to $u$
- but a single estimate $\hat{\mu}$ may be *substantial under- or over-estimate*

**Standard Error**  
$SE(\hat{u})$ tells us the **average amount** that this estimate $\hat{\mu}$ differs from the actual value of $\mu$
$$Var(\hat{\mu})=SE(\hat{\mu}) = \frac{\sigma^2}{n}$$ 
- $\sigma$ is the standard deviation of each of the realizations of $y_i$ of $Y$ 
- the more observations $n$ we have, the smaller the standard error of $\hat{\mu}$

Use standard errors to measure how close $\hat{\beta_0}$ and $\hat{\beta_1}$ to the true value $\beta_0$ and $\beta_0$ :
$$SE(\hat{\beta_0})^2=\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}], $$
$$SE(\hat{\beta_1})^2=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$Above \ \sigma^2=Var(\epsilon)$$

- $SE(\hat{\beta_1})$ is smaller when the $x_i$ are more spread out:
  - more **leverage** to estimate a slope when this is the case
- $SE(\hat{\beta_0})$ is the same as $SE(\hat{\mu})$ if $\bar{x}=0$
  - in which case $\hat{\beta_0}=\bar{y}$

In general, $\sigma^2$ is **unknown**, but can be estimated from the data
- the estimate of $\sigma$ is **known** as the **residual standard error**:
$$RSE = \sqrt{\frac{RSS}{n-2}}$$

Standard Errors can be used to compute **confidence intervals**:
- a 95% CI is defined as a range of values s.t. with 95% probability, the range will contain the **true** unknown value of the parameter.
- CI for $\beta_0 \ and \ \beta_1$
$$[\hat{\beta_i}-2·SE(\hat{\beta_i}), \hat{\beta_i}+2·SE(\hat{\beta_i})] $$
**hypothesis test**  
Null Hypothesis: No relationship between X and Y  
Alternative hypothesis: there is some relationship between X and Y
$$H_0:\beta_1=0, H_a:\beta_1\neq0$$ 
Goal: $\hat{\beta_1}$ is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero 
- how far? depends on $SE(\hat{\beta_1})$
- if $SE(\hat{\beta_1})$ is small, then even relatively small values of $\hat{\beta_1}$ may provide strong evidence that $\beta_1\neq0$
- if $SE(\hat{\beta_1})$ is large, then $\beta_1$ must be large in absolute value for us to **reject the null hypothesis**

Test: t-statistic mesures the number of standard deviations that $\hat{\beta_1}$ is away from 0
$$t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}$$
- assume no relationship between X and Y, then t-statistic will have a t-distribution
- assume $\beta_1=0$, need to compute **p-value**: the probability of observing any number equal to $|t|$ or larger in absolute value.
- a small p-value indicates that it is **unlikely**to observe such a substantial association between X and Y**due to chance(偶然）**
  - infer that there is an association between the predictor and the response.
  - reject the null hypothesis in favor of the alternative hypothesis ··· $H_a:\beta_1\neq0$
#### 3. 评价模型的准确性
To quantify the extent to which the model fits the data
- The quality of a linear regression fit is assessed using two related quantities: **$RSE$ and $R^2$**
- Table below displays three quantities for the linear regression of number of units sold on TV advertising budget

|Quantity| Value|
| --- | --- |
|RSE|3.26|
|R^2|0.612|
|F-statistic|312.1|

**Residual Standard Error - RSE**  
The RSE is an estimate of the standard deviation of $\epsilon$
- due to error terms, even if the true regression line ($\beta_0 \ and \ \beta_1$) is known, still cannot perfectly predict $Y$ from $X$
- RSE is the **average amount** that the response will **deviate** from the true regression line
$$RSE=\sqrt{\frac{RSS}{n-2}}=\sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{n-2}}$$

The RSE provides an absolute**measure of the lack of fit**of the model to the data
- if RSE is small, then the model fits the data very well
- RSE = 3.26 means actual **sales** in each market **deviate from** the true regression line by approximately 3260 units on average on the basis of **TV advertising**
- In the advertising data set, the mean value of **sales**  over all markets is about 14000 units, so the prediction error is 3260/14000 = 23%

**$R^2$ Statistic**  
Since the RSE is measured in the units of $Y$, it is NOT always clear what constitutes a **GOOD RSE**
- $R^2$ takes the form of a**porpotion**
  - the proportion of variance explained between 0 and 1
$$R^2 =\frac{TSS-RSS}{TSS}=1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y_i})^2}$$
TSS(Total Sum of Squares) measures 
- the**total variance in the response Y** before the regression is applied  

RSS measures 
- the**amount of variability** that is left **unexplained** after applying the regression

TSS-RSS measures 
- the**amount of variability** in the response that is **explained** (or removed) by performing the regression

> $R^2$ measures the proportion of variability in Y that can be explained using X

$R^2$ has an **interpretational advantage** over the RSE
- since unlike the RSE, it always lies between 0 and 1

NO GOOD $R^2$: linear model is just a rough approximation to the data and errors are often very large
- an $R^2$ value well below 0.1 might be more realistic!

$R^2$ statistic is **a measure of the linear relationship** between $X$ and $Y$, just like `correlation`
$$r=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}$$
In simple linear regression setting, we can use $r=Cor(X,Y)$ instead of $R^2=r^2$ to assess the fit of the model
- However, correlation quantifies the association between only a **single pair** of variables rather than between a larger number of variables
- Just $R^2$ applies to the multiple linear regression

#### 4. The BOOK I READ：
Introduction to Statistical Learning (ISL)


###### TOGO: 3.2. 多元线性回归
