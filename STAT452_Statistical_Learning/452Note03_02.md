## 统计学习导论(3.2)-多元线性回归
$$Y=\beta_0+\beta_1X_1+...+\beta_pX_p+\epsilon$$
Let $\beta_j$ as the average effect on $Y$ of a one unit increase in $X_j$ , **holding all other predictors fixed**
### 1. Estimating the Regression Coefficients
Estimate unknown parameters by using
$$\hat{y}=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2+...++\hat{\beta_p}x_p$$
and least squares to **minimize the RSS**(sum of squared residuals):
$$RSS=\sum_{i=1}^{n}(y_i-\hat{y_i})^2$$
$$=\sum_{i=1}^{n}(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-...-\hat{\beta_p}x_{ip})^2$$
![](https://imgkr2.cn-bj.ufileos.com/e87b2668-1e24-484e-b050-89f7c78c3cf6.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=QeL9GnTUjIJ98s%252FPjgxbvkBrFnI%253D&Expires=1600496955)

**Difference between simple and multiple linear regression:**  
In simple linear regression, we will observe that higher `newspaper` tend to be associated with higher `sales`
- HOWEVER, after adjusting for `TV` and `Radio`, the multiple regression implies the opposite:
- The table below reports the **partial effect** of adding that variable to the model.
  - p-values indicate that `TV` and `Radio` are related to `sales`
  - `newspaper` is no longer significant, in the presence of these two

![](https://imgkr2.cn-bj.ufileos.com/30288444-7c88-42de-89b9-897dba52f5b9.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=G%252BwCnA%252F9v1IKe29ZqgiG8aTY2Mc%253D&Expires=1600497056)

### 2. Four Important Questions
#### Q1: Check Relationship
Is at least one of the predivtors $X_1,X_2,...,X_p$ useful in predicting the response?  

$$H_0: \beta_1=\beta_2=···=\beta_p=0$$
$$H_a: at \ least \ one  \ \beta_j \neq 0 $$
**F-statistic**
$$F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$
  
$$TSS=\sum(y_i-\bar{y})^2, RSS=\sum(y_i-\hat{y_i})^2$$
If the linear model assumptions are correct, then $E\{RSS/(n-p-1)\} = \sigma^2$  
If $H_0$ is true, $E\{(TSS-RSS)/p\} = \sigma^2$
- F is close to 1 => no relationship  

If $H_a$ is ture, $E\{(TSS-RSS)/p\} > \sigma^2$
- F > 1  but *how large* to reject $H_0$


**Reject depends on n and p**  
When n is large, small F > 1 still provides evidence against  $H_0$
- when n is small, larger F is needed

**F-statistic is better than p-value**  
When the number of predictors p is large and $H_0: all \ \ \beta_{j>0}=0$ is true:
- We might still get **at least one** p-value below 0.05 **by chance!!**

F-statistic adjusts for the number of predictors
- therefore if $H_0$ is true, **ONLY** a 5% chance that the **overall F** will result in a p-value below 0.05 **instead of** 5% of the p-values below 0.05 associated with each variable from individual t-statistics by chance
- small F are more likely to show there is no relationship

**The Number of features: p**  
Using F to test for any association works when p << n
- If p > n, cannot even fit the multiple linear regression model ~~using least squares~~
  - F-statistic and other concepts  cannot be used
- When **p is large**, some *variable selection* approaches can be used to **high-dimensional setting**
#### Q2: Variable Selection
Do all the $X$ help to explain $Y$, or is only a **subset** of the predictors useful? (Chapter6) 
- Variable Selection: determining which predictors are associated with response, in order to fit a single model involving only those predictors

*Statistics to Judege the model quality*:
- Mallow's  $C_p$
- AIC: Akaike Information Criterion
- BIC: Bayesian Information Criterion
- Adjusted $R^2$


*Three methods* for selecting a smaller set of models (Sec6.1.2):  
1. **Forward Selection**:  
Begin with Null Model that contains an intercept but no predictors. 
- Then we fit p simple linear regressions and
- add to the null model the variable that results in the **lowest RSS** 
  - then add the lowest RSS variable for the new two-variable model
- continued until some stopping rule is satisfied
2. **Backward Selection**:  
- Problem: Backward Selection CANNOT be used if p > n, while forward selection can always be used
3. **Mixed Selection**:  
- Problem: Forward Selection is a greedy approach, and might include variables early that *later become redundant*, which Mixed Seletion can remedy. 


#### Q3: Model Fit
**How well** does the model **fit** the data?   
**$R^2$**  
In multiple linear regression, $R^2$ equals the square of the correlation between response and the fitted linear model
$$R^2=Cor(Y,\hat{Y})^2$$
- One Property of the **fitted model** is that it **maximizes** this correlation among all possible linear models
- $R^2$ = 1 indicates the model explains a large portion of the variance in the response variable
- $R^2$ will always increase when more variables are added to the model
  - even if those variables are only weakly associated with the response (`newspaper`)
  - since adding one more variable to the **least squares equations** must allow us to fit the training data more accurately
- The tiny increase in $R^2$ provides additional evidence that `newspaper` can be dropped from the model
  - `newspaper` will likely lead to poor results on independent test samples due to **overfitting**


**RSE**
$$RSE=\sqrt{\frac{RSS}{n-p-1}}$$
- Models with more variables p can have higher RSE if the decrease in RSS is small relative to the increase in p
![](https://imgkr2.cn-bj.ufileos.com/05f60e3e-ac51-410f-8814-44a5ccbd80dd.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=KxVGgdjvJ%252F4VBlQHfVwLoLN3Ndo%253D&Expires=1600497180)

Apart from RSE and $R^2$. Graphical Summaries shows:
- linear model overestimate `sales` for instances in which most of the advertising money was spent exclusively on either TV or radio
- underestimate `sales` for instances where the budget was split between the two media
- this pronounced **non-linear pattern**cannot be modeled accurately using linear regression

It suggests a **synergy** or **interaction** effect between the advertising media
- whereby combining the media together results in a bigger boost to `sales` than using any single medium
#### Q4: Prediction
Given a set of predictor values, what response value should we predict, and **how accurate** is  our **prediction**?

**Three sorts of uncertainty**:  
1. The inaccuracy in the coefficient estimates is related to the **reducible error**. We can compute a **confidence interval** in order to determine how close $\hat{Y}$ will be to $f(X)$
2. Model Bias is also apotentially reducible error.  
- So when we use a linear model, we are estimating the best linear approximation to the true surface.
- However, we will ignore this **discrepancy**, and operate **as if** the linear model were correct
3. Irreducible error lets the response value never be predicted perfectly
- Prediction Intervals answers how much will $Y$ vary from $\hat{Y}$
- PI is wider than CI, because it incorporates both the error in the estimate for $f(X)$
  - the reducible error and
  - the irreducible error: the **uncertainty** as to how much an indiviual point will differ from the population regression plane

**Confidence interval**:  
Quantify the **uncertainty** about the average `sales` over a large of cities (dataset).
### 3. Reference：
- Introduction to Statistical Learning (ISL)
- Thanks Wentian(Wendy) Wang for suggesting improvements to this note!

TOGO: 3.3. Other Considerations in the Regression Model (1) & (2)
