## 统计学习导论(3.1)-简单线性回归
> Linear Regression serves as a good jumping-off point for many fancy statistical learning approaches that can be seen as **generalizations or extensions** of linear regression

假设笔记(2.1)中的TV广告($X$)和Sales($Y$)存在线性关系:
$$Y\approx\beta_0+\beta_1X$$  
通过估计两个未知系数就可以根据已知的TV广告费预测未来的销量 $\hat{y}=\hat{\beta_0}+\hat{\beta_1}x$

### 1. 估计系数
#### 残差平方和（Residual Sum of Squares)
$$RSS= e_1^2+e_2^2+…+e_n^2$$
$$=(y_1-\hat{\beta_0}-\hat{\beta_1}x_1)^2+(y_2-\hat{\beta_0}-\hat{\beta_1}x_2)^2$$
$$+...+(y_n-\hat{\beta_0}-\hat{\beta_1}x_n)^2 = \sum_{i=1}^{n}(y_i-\hat{y_i})^2$$
**最小二乘估计**通过计算选择$\beta_0$和$\beta_1$使得RSS达到最小从而得到最优线性回归模型：
$$\hat{\beta_1}=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$\beta_0=\bar{y}-\hat{\beta_1}\bar{x}$$

![](https://imgkr2.cn-bj.ufileos.com/bce7ec97-0148-4ebf-a79c-77deed27d682.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=v67R%252FLVcAsOk0A2R65l3KXCo9lM%253D&Expires=1600124432)

#### 2. 评估系数估计值的准确性
真实的关系可能不是线性的，可能是其他变量导致了Y的变化，也可能存在测量误差。
- 通常假设误差项独立于X

![](https://imgkr2.cn-bj.ufileos.com/dd4f886a-4faf-451a-a7aa-869a98e5fe81.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=fagbVYSxO8mr1jX492%252FOCnmCkNo%253D&Expires=1600151414)

真实的**population regression line**和**least squares line**的差别原因：
- The concept of these two lines is a natural extension of the standard statistical approach of using information from a **sample** to estimate characteristics of a **large population**
- e.g. The sample mean $\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$ will provide a *good* estimate of the population mean $\hat{u}$ of variable Y

> The analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of **bias**  

**Unbiased Estimate**  
Use the sample mean $\hat{\mu}$ to estimate $\mu$
- in the sense that on average, we expect $\hat{\mu}$ to equal $\mu$
- if we average a huge number of estimates of $u$ obtained from a huge number of sets of observations, then this average would **exactly equal $\mu$**
- Hence, an unbiased estimator does not systematically **over- or under-estimate** the true parameter

The property of unbiasedness holds for the least squares coefficient estimates as well:
- average the estimates obtained over a huge number of data sets, then the average of these estimates would equal to $\beta_0$ and $\beta_1$

**Question**: how *accurate* is the sample mean $\hat{\mu}$ as an estimate of $\mu$? 
- the average of $\hat{\mu}$'s over MANY data sets will be very close to $u$
- but a single estimate $\hat{\mu}$ may be *substantial under- or over-estimate*

**Standard Error**  
$SE(\hat{u})$ tells us the **average amount** that this estimate $\hat{\mu}$ differs from the actual value of $\mu$
$$Var(\hat{\mu})=SE(\hat{\mu}) = \frac{\sigma^2}{n}$$ 
- $\sigma$ is the standard deviation of each of the realizations of $y_i$ of $Y$ 
- the more observations $n$ we have, the smaller the standard error of $\hat{\mu}$

Use standard errors to measure how close $\hat{\beta_0}$ and $\hat{\beta_1}$ to the true value $\beta_0$ and $\beta_0$ :
$$SE(\hat{\beta_0})^2=\sigma^2[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}], $$
$$SE(\hat{\beta_1})^2=\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}$$
$$Above \ \sigma^2=Var(\epsilon)$$

- $SE(\hat{\beta_1})$ is smaller when the $x_i$ are more spread out:
  - more **leverage** to estimate a slope when this is the case
- $SE(\hat{\beta_0})$ is the same as $SE(\hat{\mu})$ if $\bar{x}=0$
  - in which case $\hat{\beta_0}=\bar{y}$

In general, $\sigma^2$ is **unknown**, but can be estimated from the data
- the estimate of $\sigma$ is **known** as the **residual standard error**:
$$RSE = \sqrt{\frac{RSS}{n-2}}$$

Standard Errors can be used to compute **confidence intervals**:
- a 95% CI is defined as a range of values s.t. with 95% probability, the range will contain the **true** unknown value of the parameter.
- CI for $\beta_0 \ and \ \beta_1$
$$[\hat{\beta_i}-2·SE(\hat{\beta_i}), \hat{\beta_i}+2·SE(\hat{\beta_i})] $$
**hypothesis test**  
Null Hypothesis: No relationship between X and Y  
Alternative hypothesis: there is some relationship between X and Y
$$H_0:\beta_1=0, H_a:\beta_1\neq0$$ 
Goal:$\hat{\beta_1}$ is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero   
Test: t-statistic mesures the number of standard deviations that $\hat{\beta_1}
$$t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}$$
#### 3. 评价模型的准确性
The quality of a linear regression fit is assessed using two related quantities: $RSE$ and $R^2$
- Table below displays three quantities for the linear regression of number of units sold on TV advertising budget

|Quantity| Value|
| --- | --- |
|$RSE$|3.26|
|$R^2$|0.612|
|$F-statistic$|312.1|

**Residual Standard Error - RSE**  

**$R^2$ 统计量**
A measure of the linear relationship between $X$ and $Y$, just like `correlation`
$$R^2 =\frac{TSS-RSS}{TSS}=1-\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{\sum_{i=1}^{n}(y_i-\bar{y_i})^2}$$
总平方和(Total Sum of Squeares)
#### 4. 总结
###### TOGO: 3.2. 多元线性回归
