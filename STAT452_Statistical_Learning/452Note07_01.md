## ISLR(7)- 非线性回归分析
#### 多项式回归和阶梯函数


Note Summary：   
0.从理想的线性到现实的非线性  
1.多项式回归  
2.Step Function   
3.参考
### 0. Moving Beyond Linearity
相较于其他模型, 线性模型更易于描述和实现
- 解释性能和推断理论更有优势  

*However*, standard linear regression can have *significant limitations* in terms of *predictive power*
- Since the *linearity assumption* is  always an *poor approximation*
- Recall that *Least Squares can be improved* by Ridge Regression, LASSO, PCR... to *reduce the complexity* of the linear model
  - reduce the variance of the estimates

#### Goals Beyond Linearity：  
*Relax the Linearity assumption* while
- still *maintaining interpretability* as much as possible
#### Extensions of linear models
1. Polynomial Regression(7.1)
2. Step Function(7.2)
3. Regression Spline(7.4)
4. Smoothing Spline(7.5)
5. Local Regression(7.6)
- above approaches are for modeling the relationship between a response Y and *a single predictor X* in a flexible way.  

6. Generalized Additive Model (GAM)
- above approaches can be seamlessly integrated to model $Y$ and several $X_p$
### 1. Polynomial Regression
> Polynomial Regression extends the linear model by *adding extra predictors*, obtained by raising each of the original predictors to a power:
> - A *Cubic regression* uses three variables, $X, X^2, X^3$, as predictors to *provide a non-linear fit* to data  

Standard Linear Model to Polynomial
$$y_i = \beta_0 + \sum_{j=1}^d\beta_jx_i^j+\epsilon_i$$
- for large enough **degree d**, polynomial regression produces an *extremely non-linear curve*
- the coefficients $\beta_0,...,\beta_d$ are still estimated by Least Squre
- Genearlly, **$d \leq4$** since large d will lead polynomial curve *overly flexible* and *take strange shapes*
#### Wage & Age Non-Linear Relation
Fitting a degree-4 polynomial using least squares
- the individual coefficients are not of particular interest (black box???)
- Let $x_0$ be the value of `age`, to predict `wage`:

$$\hat{f}(x_0)=\hat{\beta}_0+\sum_{d=1}^4\hat{\beta}_dx_0^d$$

**Variance**  
Compute Variance of the fit, *$Var(\hat{f}(x_0))$*, we need:
- Variance Estimates for each of the fitted coefficients $\hat{\beta}_j$ from Least Squares
- The Covariances between pairs of coefficient estimates,
  - Let $\hat{C}$ be the 5x5 covariance matrix of the $\hat{\beta}_{j=0,1,2,3,4,}$
- Let $X_0^T=(1,x_0,x_0^2,x_0^3,x_0^4)$：
  
$$Var[(\hat{f}(x_0))]=X_0^T\hat{C}X_0$$
$\sqrt{Var(\hat{f}(x_0))}$ is the *estimated pointwise standard error* of $\hat{f}(x_0)$
- As *EACH reference point* $x_0$, this computation is repeated and get the fitted curve and twice the standard error

The pair of dotted curves at both sides of the fit are (2x) standard error curves
- Since this *(2x) quantity corresponds to an approximate 95% CI*,  for normally distributed error terms

![](https://imgkr2.cn-bj.ufileos.com/0d0904b3-5624-47b8-a70b-824e54fab18c.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=x3xNwBG%252BQAzyBoTgid%252F5Jny%252Ftqo%253D&Expires=1603255690)

**Logsitic Regression**  
We can treat `Wage` as a binary variable by splitting it into **high/low earners**
- logistic regression can be fitted to predict binary response:
$$P(y_i>250 |x_i)=\frac{e^{(\beta_0+\beta_1x_i^1+··+\beta_dx_i^d)}}{1+e^{(\beta_0+\beta_1x_i^1+··+\beta_dx_i^d)}}$$

 
Although the sample size is n = 3000, there are only 79 high earners,
- this results  in a *high variance* in the estimated coefficients and therefore fairly *wide confidence intervals*
### 2. Step Function
Using polynomial functions in a linear model imposes a **global structure** on the *non-linear* function of X
- use step function to avoid such global structure
> Step Function *cut the range of a variable* into K distinct regions to *produce a qualitative variable*
> - this has the effect of *fitting a piecewise constant function* in each bin
> - and convert a continuous variable into *ordered categorical variable*

Create cutpoints $c_1,c_2,...,c_K$ in the range of X, and then construct $K+1$ new variables:

$$\begin{aligned}
C_0(X) &= I(X < c_1) \\
C_1(X) &= I(c_1 \leq X < c_2) \\
C_2(X) &= I(c_2 \leq X < c_3) \\
       &\;\;\vdots\\
C_{K-1}(X) &= I(c_{K-1} \leq X < c_K) \\
C_K(X) &= I(c_K \leq X)       
\end{aligned}$$

Since $X$ must be in exactly one of the $K+1$ intervals, $\sum_{i=0}^KC_i(X)=1$
- Use Least Squares to fit a linear model by using $C_1(X), ..., C_K(X)$ as predictors:

$$y_i=\beta_0+\beta_1C_1(x_i)+...+\beta_KC_K(x_i)+\epsilon_i$$
- $\beta_0$  can be interpreted as the mean value of $Y$ for $X < c_1$
- $\beta_j$ can represent the average increase in the response for $X$ in $c_j \leq X < c_{j+1}$relative to $X < c_i$  

![](https://imgkr2.cn-bj.ufileos.com/b3cd8bc6-1011-4408-8115-8994ff2db1df.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=tr3ypFAWpMU%252F0jFBaJQNUGB8PVs%253D&Expires=1603255781)


Fit the Logistic Regression Model to predict the probability:

$$P(y_i>250 |x_i)=\frac{e^{\beta_0+\beta_1C_1(x_i)+...+\beta_KC_K(x_i)}}{1+e^{\beta_0+\beta_1C_1(x_i)+..+\beta_KC_K(x_i)}}$$




**Disadvantages:**  
Unless there are natural breakpoints in the predictors, piecewise-constant functions can **miss the action** 
- `age` from 20 to 30

**Advantages:**  
Step functions are more likely used in biostatistics and epidemiology,
- 5-year age groups are often used to define the bins
### 3. 参考：
- 《Introduction to Statistical Learning》 
  - Section 7.1, 7.2

 
 TOGO: (7) Basis Functions and Splines!
