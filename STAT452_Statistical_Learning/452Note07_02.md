## 统计学习12 - 基函数&回归样条
要点：  
1.基函数  
2.回归样条  
-- 分段多项式  
-- 约束条件与样条  
-- 样条基函数  
-- 确定结点的个数与位置  
-- 回归样条与多项式回归的对比  

### 1. Basic Functions
The Basic Functions are a family of transformations $b_1(·), b_2(·), ...,b_k(·)$ that can be applied to a variable $X$ to fit a non-linear model (e.g. Polynomial and Piecewise-constant regression):
$$y_i = \beta_0 + \beta_1b_1(x_i) + ... + \beta_Kb_K(x_i)+\epsilon_i$$
Basic functions are fixed and known:
- polynomial: $b_j(x_i)=x^j_i$
- piecewise: $b_j(x_i)=I(c_j\leq x_i< c _{j+1})$

We can still use least squares to estimate the unknown regression coefficients and all of the *inference tools* like MSE, F-statistics for the linear model's overall significance are available

### 2. Regression Splines
To extend upon the polynomial and piecewise constant regression, there is a flexible class of basis functions
#### 2.1 Piecewise Polynomials
**Idea**: Fitting *separate low-degree* polynomials over different region of $X$ to avoid fitting a *high-degree polynomial* over entire range of $X$

**Knots**: the points where the coefficients change
- Using *more Knots* leads to a *more flexible* piecewise polynomial 
- $K$ knots will end up fitting $K+1$ different polynomials
- Degrees of Freedom = $(d+1) \times (K+1)$

Example with $d = 3 \ \& \ K =1$:
$$y_i=
\begin{cases}
\beta_{01}+\sum_{d=1}^3\beta_{d1}x_i^d + \epsilon_i& \text{if  } x_i < c;\\
\beta_{02}+\sum_{d=1}^3\beta_{d2}x_i^d + \epsilon_i& \text{if } x_i \geq c
\end{cases}$$
#### 2.2 Constraints and Splines
Each constraint effectively frees up one degree of freedom
- by reducing the complexity of the resulting piecewise polynomial fit

![](https://imgkr2.cn-bj.ufileos.com/01a99b09-282f-4fde-b623-b1edb706d102.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=o2JIY8eMhnp3M2k3IzfEcZ%252FM4w4%253D&Expires=1604996627)

**Definition of a Degree-d Spline** : A piecewise degree-d polynomial with *continuity in derivatives up* to degree **d-1** at each knot:
- A Cubic Spline needs both 1st and 2nd derivatives are continuous at the knot
- A Cubic spline with $K$ knots uses a total of $4+K$ Degrees of Freedom
#### 2.3 The Spline Basis Representation
A cubic spline with $K$ knots can be modeled as:
$$y_i = \beta_0 + \sum_{d=1}^{K+3}\beta_db_d(x_i)+\epsilon_i$$
Basis Functions $b_d(·)$ are chosen ahead of time and the model can be fit using least squares
- the most direct way is to start off with a basis for a cubic polynomial $x, x^2, x^3$,
- and then add one **truncated power basis** function *per knot $\xi$ (/ksi/)*:
$$h(x,\xi) = (x-\xi)^3_+=
\begin{cases}
(x-\xi)^3, \  \text{if  } x > \xi\\
0, \ \  \text{otherwise} 
\end{cases}$$
In order to *fit a Cubic Spline* to a data set with $K$ knots, we perform Least Squares regression with an *intercept* and **$3+K$ predictors**: 
- $X, X^2, X^3, h(X, \xi_1),..,h(X,\xi_K)$
- **$K+4$ Degrees of Freedom**

**Disadvantages:**  
Splines can have **high variance** when $X$ takes very small or large value
- *Boundary* is the region where $X$ is *smaller* than the smallest knot or *larger* than the largest knot

![](https://imgkr2.cn-bj.ufileos.com/5ac7eca0-8e12-4400-a571-153d1f384659.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=KO48Nm8fHzwqItWUby5ySVpXxSA%253D&Expires=1604995639)
A *Natural Spline* is a regression spline with *additional boundary constraints*:
- extrapolate linearly beyond the boundary knots
- the function is required to be *linear* at the *boundary*
- which produces more stable estimates at the boundaries (CIs are narrower)

#### 2.4 Knots Placement & Selection
**Placement**  
In theory, place *more knots* (flexibility) over regions where the function seems to be *changing rapidly*,
- and place *fewer knots* where $f$ appears more *stable*

In practice it is common to place knots in a **uniform fashion**
- by specifing the desired degrees of freedom first
- and use software automatically place the corresponding number of knots at *uniform quantiles* of the data

![](https://imgkr2.cn-bj.ufileos.com/2c92bef5-58fa-4111-8019-88de3bbfe336.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=Si6kVrOV9%252BhffFJ1MhWZXDl%252Behk%253D&Expires=1604996570)
The 3 knot locations were chosen automatically as the 25th, 50th, 75th percentiles 
- by requesting 3+1 = 4 degrees of freedom

**Use CV to select the best DF**  
*Remove a portion* of the data (say 10%), fit a spline with a certain number of knots to the remaining data,
- and then use this spline to make predictions for the *held-out portion*
- Repeat until each observation has been *left out once*, then compute the *overall cross-validated RSS*

![](https://imgkr2.cn-bj.ufileos.com/d3b5075a-40e8-4472-8797-9c4cb2fbf147.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=Hl1EeQbSShvNbfwYAI8DMG09kE4%253D&Expires=1604996658)
The procedure can be repeated for different numbers of knots $K$. 
- Then the value of $K$ giving the smallest RSS is chosen


#### 2.5 Comparison to Polynomial Regression
Regression Splines are often better than polynomial regression.
- polynomials must use a high degree to produce flexible fits
- splines introduce flexibility by *increasing knots but keeping the degree fixed*
  - more stable estimates
  
![](https://imgkr2.cn-bj.ufileos.com/9579775f-9ce7-48cc-8379-0c3af4ab2e2b.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=JxhiXDdjaMmXxN3odSrA6TAZmMY%253D&Expires=1604996013)
### 3. Summary
|Model|# knots|Degrees of Freedom| 
|---|---|---|
|Regression with p predictors|0|p(+1)| 
|Degree-d Poly.Regression|0|d(+1)| 
|Cubic Spline|K|K+3(+1)| 
|Natural Cubic Spline|K|K+1(+1)| 
|Degree-d Spline|K|K+d(+1)| 
- (+1) if we count intercept



### 4. Reference
An Introduction to Statistical Learning, with applications in R (Springer, 2013)
