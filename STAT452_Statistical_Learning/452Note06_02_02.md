## ISLR(6.2)- 正则化
#### Ridge & Lasso模型对比与最优参数选择


笔记要点：   
1.Ridge & Lasso 几何解释  
2.Ridge & Lasso 原理对比   
3.特例：当数据量与特征量相等时 (n==p)  
4.贝叶斯解释  
5.选择调节参数  
6.参考
### 1. Ridge & Lasso 几何解释

The Lasso and Ridge Regression coefficient estimates can solve:

$${\min_{\beta}\{\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij})\}}, $$
$$\text{ subject to }\sum_{j=1}^p|\beta_j| \leq s, \text{ and }  \sum_{j=1}^p\beta_j^2 \leq s$$
- For *every value of $\lambda$*, there is *some s* such that above equations will give lasso & ridge coefficient estimates 
- **$s$ 控制** $\sum_{j=1}^p|\beta_j|$ 和 $\sum_{j=1}^p\beta_j^2$ 来寻找在约束下最小的RSS
- **当 $s$ 很大**, 约束降低, 系数可以很大
  - 最小二乘系数有可能落入限制区域中
  - Lasso & Ridge的估计结果等同于最小二乘的估计结果（下图）
- **当 $s$ 很小**, 那么 $\sum_{j=1}^p|\beta_j|$ 和 $\sum_{j=1}^p\beta_j^2$ 需要足够小(不超过 $s$ )的条件下尽可能寻找使得**RSS 最小**的系数

![](https://imgkr2.cn-bj.ufileos.com/4bac322b-5799-42ef-85e6-fdc0f74a422b.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=mo9hsWVQ%252F6IzuM1WohBLQyGyHOY%253D&Expires=1603071803)



以最小二乘估计系数 $\hat{\beta}$ 为中心的椭圆代表RSS值
- all of the points on a given ellipse share a common value of the RSS
- as the ellipses *expand* away from the least squares coefficient estimates, the RSS *increases*
  - 牺牲偏差增加换取方差大幅减小

Lasso和岭回归**系数估计**是由其条件区域(constraint region)与椭圆第一次**相交点**所决定 ==> (满足RSS最小)
- Since ridge regression has a circular constraint **with no sharp point**
  - this intersection will *NOT* generally occur on an axis 
- While Lasso constraint has **corners** at each of the axes
  - so the ellipse (contour of the RSS) will often intersect the constraint region at an axis (one of the coefficients = 0)
  - In higher dimensions (p>3), the constraint for the LASSO becomes a polytope, many of the coefficient estimates may equal to zero simultaneously at the intersection point.

### 2. Ridge & Lasso 原理对比
*Lasso优势：* 可以得到只包含部分变量的简单易解释模型  
*Ridge优势：* 在每个预测变量都与响应变量相关对前提下，岭回归的方差稍小, 使得最小MSE稍小于LASSO
- Lasso潜在地假设了一些系数的真值为0  

![](https://imgkr2.cn-bj.ufileos.com/9ce7c61c-b434-4684-a7df-da1d48ec6bc9.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=nZHtH3Ti4DTexj8GjEgcSfulESE%253D&Expires=1603071985)


When the response is a function of ONLY 2 out of 45 predictors
- the LASSO tends to outperform ridge regression in terms of **bias, variance, and MSE**

![](https://imgkr2.cn-bj.ufileos.com/50cc6412-dff2-4fae-8f02-af2194c5cc34.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=n0l%252F5cenz%252FDDvZcfibJ42Ghsnf0%253D&Expires=1603072111)

- 当只有一小部分预测变量是真实有效的而其他预测变量系数非常小或者等于0时， *LASSO Wins！*

- 当响应变量是很多(甚至全部的)预测变量的函数，*Ridge Wins!*
### 3. 特例：当数据量与特征量相等时 (n==p)
Let $X$ be a *diagonal matrix* with 1's on the diagonal and 0's in all off-diagonal elements, consider regression without an intercept, find $\beta_i ,..,\beta_p$ that minimize
$$\sum_{j=1}^p(y_j-\beta_j\times1)^2$$
$$Solution: \hat{\beta}_j=y_j$$

**Two types of shrinkage**  
Ridge & LASSO amounts to finding $\beta_i$ by minimizing:
$$\sum_{j=1}^p(y_j-\beta_j\times1)^2+
\begin{cases}
\lambda\sum_{j=1}^p\beta_j^2 & \text{Ridge} \\
\lambda\sum_{j=1}^p|\beta_j| & \text{Lasso} \\
\end{cases}$$
Solutions:
$$\hat{\beta}_j^R = y_j/(1+\lambda)$$
$$$$

$$\hat{\beta}_j^L = \begin{cases}
y_j-\lambda/2 & \text{if $y_j>\lambda/2$;} \\
y_j+\lambda/2 & \text{if $y_j<-\lambda/2$;} \\
0  & \text{if $|y_j|\leq\lambda/2$;}
\end{cases}$$

![](https://imgkr2.cn-bj.ufileos.com/cdfc8b2d-066f-4688-8651-e8fd7c13d03c.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=edV0m66ywBvgNMi47nLEJNZt2lk%253D&Expires=1603072196)

**Main Idea**：
- *Ridge Regression* shrinks every dimension of the data *by the same proportion*
- *LASSO* shrinks all coefficients toward zero *by a similar amount - $\lambda/2$* 
  - sufficiently small *(less than $\lambda/2$)* coefficients are all the way to zero
    - **Soft-thresholding**
### 4. 贝叶斯解释
对于回归, 贝叶斯理论假设回归系数向量 $\beta = (\beta_0, \beta_1, ... ,\beta_p)^T$ 具有先验分布(prior distribution) => *$p(\beta) = p(\beta|X)$*
- The Likelihood of the data can be written as *$f(Y|X,\beta)$*, where $X = (X_1,..,X_p)$
- Multiplying the prior distribution by the likelihood gives us (up to a proportionality constant)**posterior distribution**:
$$p(\beta|X,Y) \propto  f(Y|X,\beta)p(\beta|X)$$  
  - 比例服从贝叶斯定理

**密度函数g**  
假设 *$p(\beta)=\prod_{j=1}^pg(\beta_j)$*, 岭回归和LASSO的密度函数g服从以下两种情况：
1. If **$g$ is a Gaussian Distribution**, with mean 0 and standard deviation a function of $\lambda$
- it follows the **posterior mode** for $\beta$ is the ridge regression solution, that is the most likely value for $\beta$, given the data
- In fact, the Ridge Regression solution is also the **posterior mean**


2. If **$g$ is a Laplace(double-exponential) Distribution**, with mean 0 and scale parameter a function of $\lambda$
- it follows the **posterior mode** for $\beta$ is the LASSO solution
- However, the LASSO solution is **NOT the posterior mean**
- In fact, the posterior mean DOES *NOT yield a sparse coefficient vector*

![](https://imgkr2.cn-bj.ufileos.com/f08e63e3-ea94-42ce-bdac-f1c36a7753f0.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=TEwpYoO1x3uSypaw2H7zmffnrIY%253D&Expires=1603072264)

从贝叶斯角度, Ridge & LASSO 和普通线性模型相同, 误差正态, 同样假设 $\beta$ 具有简单先验分布
- LASSO prior is **steeply peaked** at zero
  - expects a priori that many of the coefficients are exactly zero
- Ridge prior is **flatter and flatter** at zero
  - assumes the coefficients are *randomly distributed* about zero

【简单解释】
- L1正则化相当于对模型参数 $\beta$ 引入了拉普拉斯先验, 
- L2正则化相当于引入了高斯先验
- 而拉普拉斯先验使参数为0的可能性更大
### 5. 选择调节参数(6.2.3)
To select the *best tuning parameter $\lambda$*, CV provides a simple way to tackle this problem
- choose a grid of $\lambda$ values, and compute the CV Error for each value of $\lambda$
- select the $\lambda \ $  *with the smallest CV Error*
- Finally, the model is re-fit using all of the available observations and *the selected value of the tuning parameter*

---
**LOOCV on Ridge from Credit Data:**
![](https://imgkr2.cn-bj.ufileos.com/dbcc5eb8-194f-43ac-bf72-fc9ab1fdd078.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=u3n%252FA8x9VGBu4VSYd5IDwZeus00%253D&Expires=1603072406)
In Credit dataset, the *$\lambda$ is relatively small* so that the optimal fit only involves a *small amount of shrinkage* relative to the *least squares solution*

The dip at the beginning is not very pronounced, so there is rather *a wide range of values* that would give very *similar error*
- in this case we might simply use the **least squares solution**
---
**10-fold CV on LASSO:**
![](https://imgkr2.cn-bj.ufileos.com/29ed0f2d-8e97-412b-8907-fb51534d1f78.jpeg?UCloudPublicKey=TOKEN_8d8b72be-579a-4e83-bfd0-5f6ce1546f13&Signature=Ph2Dv7%252FoIH3CDqZQ9zJV2lTYSr8%253D&Expires=1603072552)
Not only has the *LASSO correctly* given much larger coefficient estimates to the *two signal predictors*, 
- but also the minimum CV Error corresponds to a set of coefficient estimates for which *only the signal variables are non-zero*
- even though this is a challenging setting, with *p = 45 and n = 50*
  - 最小二乘结果中, 一个信号变量趋近0
    
### 6. 参考：
- 《Introduction to Statistical Learning》 
- 《老董聊卡》
- 《百面机器学习》
 
 TOGO: (7.1) Moving Beyond Linearity!

---
### Andrew Ng's Lecture
- How to solve overfitting?
  - 丢弃一些不能帮助我们正确预测的特征, 手工选择或者模型选择的算法 (i.e.PCA)
  - 正则化: 保留所有特征, 减少参数$\beta_i$的大小
    - 尤其是多项式回归中的高次项系数
- Why regularization reduces overfitting?
### 4. 总结
正则化处理: 收缩方法与边际化
- 频率视角：
  - 正则化对作用是抑制过拟合
  - 正则化项的作用是对解空间添加约束
- 贝叶斯视角：
  - 边际化天然具有模型选择功能
  - 边际化对未知对参数和超参数进行积分以消除它们的影响
