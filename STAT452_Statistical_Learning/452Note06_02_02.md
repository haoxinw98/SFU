
---
# 尝试变成下一讲
### 3. Ridge & Lasso 对比
#### Lasso的变量特征选择
# 图6.7
#### 贝叶斯解释
- Lasso倾向于得到


### Andrew Ng's Lecture
- How to solve overfitting?
  - 丢弃一些不能帮助我们正确预测的特征, 手工选择或者模型选择的算法 (i.e.PCA)
  - 正则化: 保留所有特征, 减少参数$\beta_i$的大小
    - 尤其是多项式回归中的高次项系数
- Why regularization reduces overfitting?
### 4. 总结
正则化处理: 收缩方法与边际化
- 频率视角：
  - 正则化对作用是抑制过拟合
  - 正则化项的作用是对解空间添加约束
- 贝叶斯视角：
  - 边际化天然具有模型选择功能
  - 边际化对未知对参数和超参数进行积分以消除它们的影响
