## ISLR 7.7 广义可加模型
要点：  
0.广义可加模型介绍  
1.用于回归问题的GAM   
-- 多元线性回归的推广    
2.用于分类问题的GAM   
-- 逻辑回归的推广  
3.GAM的优点与不足
### 0. Generalized Additive Models
Polynomials, Step functions & Splines can be seen as extensions of *simple linear regression*
- flexibly predicting $Y$ on the basis of a *single predictor* $X$

**Generalized Additive Models (GAMs)** provide a general framework for extending *multiple linear model*
- by allowing non-linear functions for $X_1,...,X_p$ while *maintaining additivity*
### 1. GAMs for Regression
Recall Multiple Linear Regression:
$$y_i=\beta_0+\sum_{j=1}^p\beta_ix_{ij}+\epsilon_i$$

GAMs replace each *linear* component $\beta_jx_{ij}$ with a *non-linear(smooth)* function $f_j(x_{ij})$:
$$y_i=\beta_0+\sum_{j=1}^pf_j(x_{ij})+\epsilon_i$$
- It is called an **addictive model** because we calculate a *separate $f_j$* for each $X_j$, 
- and then *add together all* of them.

>The beauty of GAMs is that we can use different methods *as building blocks* for fitting an *additive model*
#### Example：
To predict the `wage`, fitting the model:
$wage=\beta_0+f_1(year)+f_2(age)+f_3(education)+\epsilon$
- `year` and `age` are quantitative, fit both functions using *natural splines*
- `education` is qualitative with 5 levels: *<HS, HS, <Coll, Coll, >Coll*, fit *step function* for each level via the *dummy variable*.

Since *natural splines* can be constructed using an pre-chosen set of basis functions
- the entire model is just a big regreesion onto *dummy and spline basis variables* using least squares
# figure11

Using *Smooth Splines* as the building blocks looks rather similar
- fitting via *backfitting* instead of ~~*least square*~~
# figure12
### 2. GAMs for Classification 
Assume $Y$ takes on values {0,1}
- Let $p(X)=\Pr(Y=1|X)$

Recall Logistic Regression:
$$\log(\frac{p(X)}{1-p(X)})=\beta_0+\sum_{i=1}^p\beta_iX_i$$
- This logit is the *log of the odds* of $P(Y=1|X)$ versus $P(Y=0|X)$, which represents as a linear function of the predictors.

Logic Regression Extension GAM for non-linear relationships:
$$\log(\frac{p(X)}{1-p(X)})=\beta_0+\sum_{i=1}^pf_i(X_i)$$

#### Example:
To predict the probability that an individual's income exceeds $25,000 per year, fitting the GAM:

$\log(\frac{p(X)}{1-p(X)})=\beta_0+\beta_1\times year+f_2(age)+f_3(education),$
where $p(X)=\Pr(wage>250\ |\ year,age,eduction)$
- $f_2$ is fit via *smoothing spline with DF=5*
- $f_3$ is fit as a *step function* via dummy variables for levels of `education`
# fig13
- Fact: no one *<HighSchool* make *wage>250*
  - CI very wide
  - need refit the GAM without *<HS* level
### 3. Pros and Cons of GAMs
#### Advantages:
1. GAMs allow us to fit a non-linear $f_j$ to each $X_j$ automatically
- standard linear regression will miss this
2. The Non-linear Fits potentially make more *accurate predictions* for the response $Y$
3. Because the model is *additive*, we can 
- still *examine the effect* of each $X_j$ on $Y$ *individually* while holding all of the *other variables fixed*
4. The *smoothness* of the function $f_j$ for the variable $X_j$ can be summarized via *degree of freedom*
#### Limitations:
1. The model is *restricted to be additive*
2. Important *interactions* with many variables can be *missed*
- need to manually add $f_{jk}\ (X_j \times X_k)$
3. TOGO: For fully general models without limitations, we will look for even more flexible methods:  
- Random Forest, Boosting


### 4. Reference
An Introduction to Statistical Learning, with applications in R (Springer, 2013)
